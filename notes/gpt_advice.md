gpt advice
Below is a complete ‚ÄúHey Chef‚Äù codebase rewritten to use Picovoice Porcupine for wake-word detection instead of Snowboy. It includes all necessary Python files, a Streamlit front-end, and step-by-step instructions for acquiring Porcupine models and installing dependencies.

‚∏ª

üìÅ Project Structure

Create a new folder (e.g. hey_chef/) and inside it place the following files and folders exactly as shown:

hey_chef/
‚îú‚îÄ‚îÄ wake_porcupine.py
‚îú‚îÄ‚îÄ stt_whisper.py
‚îú‚îÄ‚îÄ llm_client.py
‚îú‚îÄ‚îÄ tts_engine.py
‚îú‚îÄ‚îÄ streamlit_app.py
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ porcupine_models/
    ‚îú‚îÄ‚îÄ hey_chef.ppn
    ‚îî‚îÄ‚îÄ porcupine_params.pv

	‚Ä¢	porcupine_models/hey_chef.ppn ‚Äì Your custom wake-word model for ‚ÄúHey Chef‚Äù (downloaded from Picovoice Console).
	‚Ä¢	porcupine_models/porcupine_params.pv ‚Äì The Porcupine engine parameters file (comes with Porcupine SDK).
	‚Ä¢	All other .py files are fully self-contained modules you‚Äôll copy-paste from this guide.
	‚Ä¢	requirements.txt lists required Python packages with suggested version pins.

‚∏ª

1. Obtain & Place Porcupine Models

1.1. Sign up & Create a Wake-Word Model
	1.	Go to https://console.picovoice.ai/ and sign up (it‚Äôs free for personal/hobby use).
	2.	Create a new Porcupine project and add a custom wake word ‚Äúhey chef‚Äù (make sure to type lowercase, e.g. hey chef).
	3.	Download the generated model (hey_chef.ppn) and the Porcupine parameters file (porcupine_params.pv). You can download them both from the ‚ÄúModels‚Äù tab of your project.
	4.	Place those two files into a folder called porcupine_models/ under your project root:

hey_chef/
‚îî‚îÄ‚îÄ porcupine_models/
    ‚îú‚îÄ‚îÄ hey_chef.ppn
    ‚îî‚îÄ‚îÄ porcupine_params.pv



Note:
	‚Ä¢	In free (Hobby) tier, your model will expire every 30 days; you need to re-download and replace the .ppn file to continue using it.
	‚Ä¢	If you ever need a new wake-word or higher usage limits (beyond 3 active users/month), you‚Äôll need a paid plan.

‚∏ª

2. Install System Dependencies

On macOS, you‚Äôll need PortAudio for microphone capture (through PyAudio) and the Porcupine Python SDK.
	1.	Install Homebrew (if you don‚Äôt already have it):

/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"


	2.	Install PortAudio:

brew install portaudio


	3.	Install Python 3.10+ (if not already installed):

brew install python@3.10

	‚Ä¢	Make sure python3 and pip3 point to this version.

‚∏ª

3. Install Porcupine SDK & Python Packages
	1.	Clone & install the Porcupine Python wrapper (Picovoice Open Source):

cd hey_chef
git clone https://github.com/Picovoice/porcupine.git
cd porcupine/binding/python
pip3 install .
cd ../../../

What this does:
	‚Ä¢	Clones the official Porcupine repository and installs the Python wheel from binding/python/setup.py.
	‚Ä¢	It provides the pvporcupine module used below.

	2.	Install the remaining Python packages using requirements.txt:
Create a file requirements.txt in hey_chef/ with the following contents:

streamlit==1.30.0
pvporcupine==2.2.0
pyaudio==0.2.11
openai==0.27.0
pyttsx3==2.90
requests==2.31.0
beautifulsoup4==4.12.2

Then run:

cd hey_chef
pip3 install --upgrade pip
pip3 install -r requirements.txt

	‚Ä¢	pvporcupine is the Porcupine Python SDK.
	‚Ä¢	pyaudio lets us capture from the microphone.
	‚Ä¢	openai is the OpenAI Python client for Whisper/GPT.
	‚Ä¢	pyttsx3 is an offline TTS engine on macOS.

‚∏ª

4. Set Environment Variables
	1.	Export your OpenAI API key (replace sk-REPLACE_WITH_YOUR_KEY with your actual key):

export OPENAI_API_KEY="sk-REPLACE_WITH_YOUR_KEY"


	2.	Optionally, you can add that export line to your ~/.zshrc or ~/.bash_profile so it persists across sessions.

‚∏ª

5. Copy-Paste All Python Code Files

Below are the complete contents for each .py file. Create them under hey_chef/ exactly as named.

‚∏ª

5.1. wake_porcupine.py

# wake_porcupine.py
#
# Uses Picovoice Porcupine to detect the wake word ‚Äúhey chef‚Äù on-device.
# Requires:
#   - porcupine_models/hey_chef.ppn       (your trained wake-word model)
#   - porcupine_models/porcupine_params.pv (Porcupine engine parameters)
#   - pyaudio (for microphone capture)
#   - pvporcupine (Porcupine Python SDK)

import pvporcupine
import pyaudio
import signal
import sys
import os

class WakeWordDetector:
    """
    Blocks until the wake word ‚Äúhey chef‚Äù is detected via Porcupine.
    """

    def __init__(self,
                 keyword_path: str = "porcupine_models/hey_chef.ppn",
                 pv_params_path: str = "porcupine_models/porcupine_params.pv",
                 sensitivity: float = 0.7):
        """
        keyword_path: Path to your hey_chef.ppn
        pv_params_path: Path to porcupine_params.pv
        sensitivity: Detection sensitivity (0.0‚Äì1.0). Higher = more sensitive (more false positives).
        """
        if not os.path.isfile(keyword_path):
            raise FileNotFoundError(f"Wake-word model not found: {keyword_path}")
        if not os.path.isfile(pv_params_path):
            raise FileNotFoundError(f"Porcupine params file not found: {pv_params_path}")

        # Create the Porcupine instance
        self.porcupine = pvporcupine.create(
            keyword_paths=[keyword_path],
            model_path=pv_params_path,
            sensitivities=[sensitivity]
        )

        # Initialize PyAudio stream
        self.pa = pyaudio.PyAudio()
        self.stream = self.pa.open(
            rate=self.porcupine.sample_rate,
            channels=1,
            format=pyaudio.paInt16,
            input=True,
            frames_per_buffer=self.porcupine.frame_length
        )

        # Setup SIGINT handler for clean shutdown
        signal.signal(signal.SIGINT, self._signal_handler)

    def _signal_handler(self, sig, frame):
        """
        Invoked on Ctrl+C. Cleans up and exits.
        """
        self.cleanup()
        sys.exit(0)

    def detect_once(self) -> bool:
        """
        Blocks until the wake word is detected once. Returns True when detected.
        """
        print("üëÇ Listening for wake word (‚ÄòHey Chef‚Äô)‚Ä¶")
        while True:
            pcm = self.stream.read(self.porcupine.frame_length, exception_on_overflow=False)
            pcm_int16 = pvporcupine.util.pv_buffer_to_int16_list(pcm)
            result = self.porcupine.process(pcm_int16)
            if result >= 0:
                # result is the index of the detected keyword in keyword_paths
                print("üü¢ Wake word detected!")
                return True

    def cleanup(self):
        """
        Release resources cleanly.
        """
        if hasattr(self, "stream") and self.stream is not None:
            self.stream.stop_stream()
            self.stream.close()
        if hasattr(self, "pa") and self.pa is not None:
            self.pa.terminate()
        if hasattr(self, "porcupine") and self.porcupine is not None:
            self.porcupine.delete()


‚∏ª

5.2. stt_whisper.py

# stt_whisper.py
#
# Records a fixed-duration audio clip from the default microphone,
# writes it to a temporary WAV file, and transcribes via OpenAI Whisper.

import pyaudio
import wave
import tempfile
import openai
import os

class WhisperSTT:
    """
    Records a short audio snippet (default 6 seconds) and transcribes with OpenAI Whisper.
    """

    def __init__(self, record_seconds: int = 6, sample_rate: int = 16000, chunk: int = 1024):
        """
        record_seconds: how many seconds to record after wake word
        sample_rate: must match Whisper‚Äôs expected sample rate (16 kHz)
        chunk: frames per buffer
        """
        self.record_seconds = record_seconds
        self.format = pyaudio.paInt16
        self.channels = 1
        self.rate = sample_rate
        self.chunk = chunk
        self.pa = pyaudio.PyAudio()

        # Ensure OPENAI_API_KEY is set
        if "OPENAI_API_KEY" not in os.environ:
            raise EnvironmentError("Please set the OPENAI_API_KEY environment variable.")
        openai.api_key = os.environ["OPENAI_API_KEY"]

    def record_audio(self) -> str:
        """
        Records from the default microphone for self.record_seconds,
        saves to a temp WAV file, and returns its filepath.
        """
        stream = self.pa.open(format=self.format,
                              channels=self.channels,
                              rate=self.rate,
                              input=True,
                              frames_per_buffer=self.chunk)
        frames = []
        print(f"üé§ Recording for {self.record_seconds} seconds‚Ä¶")
        for _ in range(int(self.rate / self.chunk * self.record_seconds)):
            data = stream.read(self.chunk, exception_on_overflow=False)
            frames.append(data)
        print("üõë Recording complete.")
        stream.stop_stream()
        stream.close()

        # Write to a temp WAV
        temp_wav = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
        with wave.open(temp_wav.name, "wb") as wf:
            wf.setnchannels(self.channels)
            wf.setsampwidth(self.pa.get_sample_size(self.format))
            wf.setframerate(self.rate)
            wf.writeframes(b"".join(frames))

        return temp_wav.name

    def speech_to_text(self, wav_path: str) -> str:
        """
        Uses OpenAI Whisper (whisper-1) to transcribe the given WAV file.
        Returns the transcribed text.
        """
        with open(wav_path, "rb") as audio_file:
            transcript = openai.Audio.transcribe("whisper-1", audio_file)
        text = transcript.get("text", "").strip()
        print(f"üìù Transcribed: {text!r}")
        return text

    def cleanup(self):
        """
        Cleanly terminate the PyAudio instance.
        """
        self.pa.terminate()


‚∏ª

5.3. llm_client.py

# llm_client.py
#
# A minimal OpenAI GPT-4o client. Replace OPENAI_API_KEY in your environment.

import os
import openai

class LLMClient:
    """
    Wraps calls to OpenAI‚Äôs ChatCompletion for GPT-4o.
    """

    def __init__(self, model: str = "gpt-4o"):
        """
        model: which OpenAI model to use (e.g. "gpt-4o", "gpt-4o-mini", etc.)
        """
        if "OPENAI_API_KEY" not in os.environ:
            raise EnvironmentError("Please set the OPENAI_API_KEY environment variable.")
        openai.api_key = os.environ["OPENAI_API_KEY"]
        self.model = model

    def ask(self, recipe_text: str, user_question: str) -> str:
        """
        Sends a combined prompt (system + user) to the LLM, returns its text response.
        """
        system_msg = {
            "role": "system",
            "content": (
                "You are ChefBot, an expert cooking assistant. "
                "The user will supply a full recipe, then ask a question like 'Which ingredients do I need?' "
                "or 'How do I start?' Provide a clear, concise answer focused on cooking instructions."
            )
        }
        user_msg = {
            "role": "user",
            "content": f"Recipe:\n{recipe_text}\n\nQuestion: {user_question}"
        }

        response = openai.ChatCompletion.create(
            model=self.model,
            messages=[system_msg, user_msg],
            temperature=0.2,
            max_tokens=500
        )
        reply = response.choices[0].message.content.strip()
        return reply


‚∏ª

5.4. tts_engine.py

# tts_engine.py
#
# A simple pyttsx3-based TTS wrapper. This uses your system‚Äôs default voice.

import pyttsx3

class TTSEngine:
    """
    Wraps pyttsx3 for text-to-speech. On macOS, this uses the built-in voices.
    """

    def __init__(self, rate: int = 150, voice_id: str = None):
        """
        rate: speech rate (words per minute)
        voice_id: if you want a specific voice ID (e.g. ‚Äúcom.apple.speech.synthesis.voice.Alex‚Äù on macOS).
        """
        self.engine = pyttsx3.init()
        self.engine.setProperty("rate", rate)
        if voice_id:
            self.engine.setProperty("voice", voice_id)

    def say(self, text: str):
        """
        Speaks the given text. Blocks until speaking is done.
        """
        self.engine.say(text)
        self.engine.runAndWait()


‚∏ª

5.5. streamlit_app.py

# streamlit_app.py
#
# A minimal Streamlit front-end that:
# 1) Lets you paste a recipe and ‚ÄúSave Recipe‚Äù
# 2) Launches the ‚ÄúHey Chef‚Äù voice loop using Porcupine ‚Üí Whisper ‚Üí GPT-4o ‚Üí pyttsx3
#
# Run with: streamlit run streamlit_app.py

import streamlit as st
import threading

from wake_porcupine import WakeWordDetector
from stt_whisper import WhisperSTT
from llm_client import LLMClient
from tts_engine import TTSEngine

# -----------------------------------------------------------------------------
# 1) PAGE SETUP
# -----------------------------------------------------------------------------
st.set_page_config(page_title="Hey Chef", page_icon="üç≥", layout="centered")
st.title("üç≥ Hey Chef (Prototype)")
st.markdown(
    """
    **Step 1.** Paste or edit your recipe in the box below, then click **Save Recipe**.  
    **Step 2.** Click **Start Listening**.  
    In your terminal/console you‚Äôll see ‚ÄúListening for ‚ÄòHey Chef‚Äô‚Ä¶‚Äù.  
    **Step 3.** Say **‚ÄúHey Chef‚Äù** out loud, then ask your cooking question (e.g., ‚ÄúWhat ingredients do I need?‚Äù).  
    **Step 4.** ChefBot will reply via text-to-speech (pyttsx3) to your default audio output.
    """
)

if "recipe_text" not in st.session_state:
    st.session_state.recipe_text = ""

recipe_input = st.text_area(
    label="Paste your recipe (plain text or markdown):",
    value=st.session_state.recipe_text,
    height=300,
    placeholder=(
        "# Hearty Bean Soup  \n"
        "## Ingredients  \n"
        "- 2 cups dried beans  \n"
        "- 4 cups vegetable broth  \n"
        "- 1 onion, diced  \n"
        "## Instructions  \n"
        "1. Rinse beans and soak overnight ...  \n"
        "2. ..."
    )
)

if st.button("Save Recipe"):
    st.session_state.recipe_text = recipe_input
    st.success("‚úÖ Recipe saved! You can now click **Start Listening** below.")

# -----------------------------------------------------------------------------
# 2) VOICE LOOP (BACKGROUND THREAD)
# -----------------------------------------------------------------------------
def start_voice_loop():
    """
    Runs indefinitely:
      1) Waits for ‚ÄúHey Chef‚Äù via Porcupine
      2) Records user question (Whisper)
      3) Sends recipe + question to GPT-4o
      4) Speaks response (pyttsx3)
    """
    # 2.a) Initialize Porcupine wake-word detector
    keyword_path = "porcupine_models/hey_chef.ppn"
    pv_params_path = "porcupine_models/porcupine_params.pv"
    wwd = WakeWordDetector(keyword_path=keyword_path,
                          pv_params_path=pv_params_path,
                          sensitivity=0.7)

    # 2.b) Initialize Whisper STT
    stt = WhisperSTT(record_seconds=6)

    # 2.c) Initialize LLM client
    llm = LLMClient(model="gpt-4o")

    # 2.d) Initialize TTS
    tts = TTSEngine(rate=150)

    try:
        while True:
            # Block until ‚Äúhey chef‚Äù detected
            wwd.detect_once()

            # Now record & transcribe question
            print("üü¢ Wake word detected! Recording your question‚Ä¶")
            wav_path = stt.record_audio()
            user_question = stt.speech_to_text(wav_path)
            print(f"üëâ You asked: {user_question}")

            # Fetch saved recipe from Streamlit session
            recipe = st.session_state.get("recipe_text", "").strip()
            if not recipe:
                tts.say("I don't have a recipe yet. Please paste a recipe and save it first.")
                continue

            # Query the LLM
            answer = llm.ask(recipe, user_question)
            print(f"üç≤ ChefBot says: {answer}")

            # Speak the answer
            tts.say(answer)

    except Exception as e:
        print(f"‚ö†Ô∏è Voice loop stopped due to error: {e}")
    finally:
        stt.cleanup()
        print("üõë Voice loop terminated.")

if st.button("Start Listening"):
    thread = threading.Thread(target=start_voice_loop, daemon=True)
    thread.start()
    st.info("üîä Voice loop started in the background. Check your console for logs.")


‚∏ª

5.6. requirements.txt

streamlit==1.30.0
pvporcupine==2.2.0
pyaudio==0.2.11
openai==0.27.0
pyttsx3==2.90
requests==2.31.0
beautifulsoup4==4.12.2

Feel free to bump these versions to the latest compatible releases, but pinning ensures reproducibility.

‚∏ª

6. Detailed Setup & Execution Steps
	1.	Ensure you have Python 3.10+ installed (macOS via Homebrew is recommended).

brew install python@3.10


	2.	Install PortAudio (for pyaudio):

brew install portaudio


	3.	Clone/prepare your project folder:

mkdir hey_chef
cd hey_chef
# Then create the files above inside this folder.


	4.	Download Porcupine model files:
	‚Ä¢	Sign up at https://console.picovoice.ai/
	‚Ä¢	Create a new Wake-word ‚Äúhey chef‚Äù project
	‚Ä¢	Download hey_chef.ppn and porcupine_params.pv
	‚Ä¢	Place them under hey_chef/porcupine_models/
	5.	Install Porcupine Python SDK:

git clone https://github.com/Picovoice/porcupine.git
cd porcupine/binding/python
pip3 install .
cd ../../..

	‚Ä¢	This installs the pvporcupine package into your Python environment.

	6.	Install remaining Python dependencies:
In hey_chef/, run:

pip3 install --upgrade pip
pip3 install -r requirements.txt


	7.	Set your OpenAI API key:

export OPENAI_API_KEY="sk-REPLACE_WITH_YOUR_OPENAI_KEY"


	8.	Run the Streamlit app:

cd hey_chef
streamlit run streamlit_app.py

	‚Ä¢	Your default browser will open at http://localhost:8501.

	9.	Use the App:
	‚Ä¢	Paste or edit a recipe in the text area (plain text or Markdown).
	‚Ä¢	Click Save Recipe.
	‚Ä¢	Click Start Listening.
	‚Ä¢	Observe your terminal/console: it should print ‚ÄúListening for wake word (‚ÄòHey Chef‚Äô)‚Ä¶‚Äù.
	‚Ä¢	Say ‚ÄúHey Chef‚Äù out loud (in moderate volume, quiet environment).
	‚Ä¢	Right after, ask your cooking question (e.g. ‚ÄúWhat ingredients do I need?‚Äù).
	‚Ä¢	The app will record ~6 seconds, transcribe via Whisper, send your saved recipe + question to GPT-4o, and reply via TTS to your default audio device (headphones or speakers).
	10.	Stopping the App:
	‚Ä¢	To stop Streamlit entirely, press Ctrl+C in the console.
	‚Ä¢	The voice loop thread will also terminate at that point.

‚∏ª

7. Tips & Troubleshooting
	1.	Porcupine Model Expiration
	‚Ä¢	In the free hobby tier, your hey_chef.ppn model expires every 30 days.
	‚Ä¢	When it expires, re-login to Picovoice Console, re-download a fresh hey_chef.ppn, overwrite the file in porcupine_models/, and restart the app.
	2.	Adjusting Sensitivity
	‚Ä¢	In WakeWordDetector.__init__, we set sensitivity=0.7.
	‚Ä¢	If Porcupine never triggers, increase to 0.8 or 0.85.
	‚Ä¢	If Porcupine triggers spuriously, reduce to 0.6 or 0.5.
	3.	Audio Device Configuration
	‚Ä¢	Porcupine‚Äôs PyAudio stream picks the default system microphone.
	‚Ä¢	If you have multiple input devices (e.g. USB mic, headphone mic), you can specify input_device_index=... when opening PyAudio. Example:

self.stream = self.pa.open(
    rate=self.porcupine.sample_rate,
    channels=1,
    format=pyaudio.paInt16,
    input=True,
    input_device_index=2,  # adjust based on `pyaudio.get_device_info_by_index(i)`
    frames_per_buffer=self.porcupine.frame_length
)


	‚Ä¢	Use a small script to list all pa.get_device_info_by_index(i)["name"] and pick the index of your preferred mic.

	4.	Whisper Recording Duration
	‚Ä¢	We record 6 seconds by default.
	‚Ä¢	If your spoken question tends to be longer, you can bump WhisperSTT(record_seconds=6) to record_seconds=8 or 10.
	‚Ä¢	Each extra second of Whisper transcription costs $0.006 per minute ‚âà $0.0001 per second.
	5.	LLM Token Limits
	‚Ä¢	If your recipe is very long (>‚Äâ5‚Äâ000 tokens), you may hit the LLM‚Äôs token limit. In that case, consider sending only the relevant section (e.g. the ‚ÄúIngredients‚Äù portion) or summarising first.
	‚Ä¢	GPT-4o pricing (mid-2025):
	‚Ä¢	$2.50 per 1M input tokens
	‚Ä¢	$10.00 per 1M output tokens
	6.	pyttsx3 Voice Choices
	‚Ä¢	On macOS, pyttsx3 defaults to ‚ÄúAlex‚Äù or ‚ÄúSamantha.‚Äù
	‚Ä¢	To list voices:

import pyttsx3
e = pyttsx3.init()
for v in e.getProperty("voices"):
    print(v.id, v.name)


	‚Ä¢	To pick a specific voice, pass its voice_id into TTSEngine(voice_id="com.apple.speech.synthesis.voice.Alex").

‚∏ª

8. Final Checklist

Make sure you have:
	‚Ä¢	A folder hey_chef/ containing exactly the files listed above:
	‚Ä¢	wake_porcupine.py
	‚Ä¢	stt_whisper.py
	‚Ä¢	llm_client.py
	‚Ä¢	tts_engine.py
	‚Ä¢	streamlit_app.py
	‚Ä¢	requirements.txt
	‚Ä¢	A subfolder porcupine_models/ with hey_chef.ppn and porcupine_params.pv
	‚Ä¢	Installed PortAudio via Homebrew (brew install portaudio).
	‚Ä¢	Installed Porcupine‚Äôs Python SDK by cloning and running pip3 install . in porcupine/binding/python/.
	‚Ä¢	Installed all required Python packages:

pip3 install --upgrade pip
pip3 install -r requirements.txt


	‚Ä¢	Exported your OPENAI_API_KEY in the shell.
	‚Ä¢	Verified that running python3 -c "import pvporcupine; print('Porcupine OK')" does not error.
	‚Ä¢	Run the app with:

cd hey_chef
streamlit run streamlit_app.py



Once all of the above are in place, you will have a fully functional ‚ÄúHey Chef‚Äù assistant that:
	1.	Waits for you to say ‚ÄúHey Chef‚Äù (via Porcupine).
	2.	Records your spoken cooking question (via Whisper).
	3.	Sends your saved recipe + question to GPT-4o (via OpenAI).
	4.	Speaks back the answer (via pyttsx3 TTS).

Enjoy your Porcupine-powered Hey Chef voice assistant!